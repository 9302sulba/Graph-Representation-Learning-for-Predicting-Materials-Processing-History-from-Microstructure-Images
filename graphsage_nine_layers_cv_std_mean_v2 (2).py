# -*- coding: utf-8 -*-
"""GraphSage_nine_layers_cv_std_mean_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RZZUdIr9HU7WsZ_JCWe5gIED_g0S7d7I
"""

import pandas as pd
import torch
import igraph as ig
import os
import glob
from torch_geometric.data import Data

# Read the Excel file
df = pd.read_excel("/bsuscratch/sulbhamalviya/New_Test_Folder/Test_1/Data_NoTime.xlsx")

# Calculate the maximum values for normalization
#max_time = df['time'].max()
max_temperature = df['temperature'].max()

# Normalize 'time' and 'temperature' using the maximum values
#df['time'] = df['time'] / max_time
df['temperature'] = df['temperature'] / max_temperature

# Create a dictionary to map filenames to their target values
#file_to_targets = {row["image_name"]: [row["Cr"], row["Co"], row["temperature"], row["time"]] for _, row in df.iterrows()}
file_to_targets = {row["image_name"]: [row["Cr"], row["Co"], row["temperature"]] for _, row in df.iterrows()}

# Define the path to the GraphML files directory and process each GraphML file
graphml_dir_path = "/bsuscratch/sulbhamalviya/New_Test_Folder/Test_1/Graph_Generation/"
data_list = []

for graphml_file_path in glob.glob(os.path.join(graphml_dir_path, "*.graphml")):
    filename = os.path.basename(graphml_file_path)

    if filename in file_to_targets:
        # Load the graph using igraph
        graph = ig.Graph.Read_GraphML(graphml_file_path)

        # Extract node features
        node_features = [[vertex['value']] for vertex in graph.vs]
        edge_indices = [[edge.source, edge.target] for edge in graph.es]
        edge_attrs = [[edge['weight']] for edge in graph.es]

        # Convert to PyTorch tensors
        x = torch.tensor(node_features, dtype=torch.float)
        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()
        edge_attr = torch.tensor(edge_attrs, dtype=torch.float)

        # Get the target values for this graph
        target = torch.tensor(file_to_targets[filename], dtype=torch.float32)

        # Reshape target to [1, 4] (assuming each graph has exactly one target)
        #target_reshaped = target.view(1, 3)
        target_reshaped = target.view(-1, 3)  # Make sure the target has the shape [batch_size, 3]


        # Create PyTorch Geometric Data object
        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=target_reshaped)
        #print("data.x",data.x)
        data_list.append(data)

# Print the number of Data objects created
print(f"Number of Data objects created: {len(data_list)}")

df.head()

# from sklearn.model_selection import train_test_split
# from torch_geometric.loader import DataLoader

# # Split data_list into training and testing sets
# train_data, test_data = train_test_split(data_list, test_size=0.2, random_state=42)

# # Create DataLoader instances for training and testing data
# train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
# test_loader = DataLoader(test_data, batch_size=32, shuffle=False)

# # Print the number of training and testing samples
# print(f"Number of training samples: {len(train_data)}")
# print(f"Number of testing samples: {len(test_data)}")

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv, global_mean_pool

# Define the GraphSAGE model with weight initialization
class GraphSAGEModel(nn.Module):
    def __init__(self, in_channels, out_channels, hidden_channels=128):
        super(GraphSAGEModel, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='mean')
        self.conv2 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')
        self.conv3 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')
        self.conv4 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')
        self.conv5 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')
        self.conv6 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')
        self.conv7 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')
        self.conv8 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')
        self.conv9 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')
        self.fc1 = nn.Linear(hidden_channels, hidden_channels)
        self.fc2 = nn.Linear(hidden_channels, hidden_channels)
        self.fc3 = nn.Linear(hidden_channels, hidden_channels)
        self.fc4 = nn.Linear(hidden_channels, hidden_channels)
        self.fc5 = nn.Linear(hidden_channels, out_channels)

        # Apply weight initialization
        self.initialize_weights()

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, SAGEConv):
                # Apply Xavier initialization for GraphSAGE layers
                torch.nn.init.xavier_uniform_(m.lin_l.weight)
                torch.nn.init.xavier_uniform_(m.lin_r.weight)
                if m.lin_l.bias is not None:
                    torch.nn.init.zeros_(m.lin_l.bias)
            elif isinstance(m, nn.Linear):
                # Apply He initialization for fully connected layers
                torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')
                if m.bias is not None:
                    torch.nn.init.zeros_(m.bias)

    def forward(self, x, edge_index, edge_attr, batch):
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = F.relu(self.conv3(x, edge_index))
        x = F.relu(self.conv4(x, edge_index))
        x = F.relu(self.conv5(x, edge_index))
        x = F.relu(self.conv6(x, edge_index))
        x = F.relu(self.conv7(x, edge_index))
        x = F.relu(self.conv8(x, edge_index))
        x = F.relu(self.conv9(x, edge_index))
        x = global_mean_pool(x, batch)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.relu(self.fc4(x))
        x = self.fc5(x)  # No activation in the final layer
        return x

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error

def parity_plot(ground_truth, prediction,
                lims_min, lims_max,
                plot_name, r2_value=None,
                ground_truth_norm=None, prediction_norm=None,mse_mean=None, mse_std=None,
                r2_mean=None, r2_std=None,label=None):
    # plt.figure()
    plt.figure(figsize=(3.5,3.5), dpi=200)

    # Scatter plot of true vs predicted values
    plt.scatter(ground_truth, prediction,  color='black', s=5)

    # Define the line for perfect prediction
    plt.plot([lims_min, lims_max], [lims_min, lims_max], 'k--',linewidth=1)

    # Set limits for x and y axes to the range of your data
    plt.xlim(lims_min, lims_max)  # X-axis limit
    plt.ylim(lims_min, lims_max)  # Y-axis limit

    # Add labels, title, legend, and grid
    plt.xlabel('Ground Truth', fontsize=25, labelpad=4)
    plt.ylabel('Prediction', fontsize=25, labelpad=4)
    plt.title(label, fontsize=20,loc='center')
    #plt.legend().remove()
    plt.grid(False)
    plt.tick_params(axis='both', which='both', length=0)

    # Calculate MSE
    # mse = mean_squared_error(ground_truth, prediction)
    # Use normalized values if provided, otherwise fall back to original values
    gt_mse = ground_truth_norm if ground_truth_norm is not None else ground_truth
    pred_mse = prediction_norm if prediction_norm is not None else prediction
    mse = mean_squared_error(gt_mse, pred_mse)

    if mse_mean is not None and mse_std is not None and r2_mean is not None and r2_std is not None:
        #text_str = f'$\\bf{{R^2}}$ = {r2_mean:.4f} ± {r2_std:.4f}\n$\\bf{{MSE}}$ = {mse_mean:.1E} ± {mse_std:.1E}'
        text_str = f"MSE  = {mse_mean:.1E} ± {mse_std:.1E}\nR²   = {r2_mean:.4f} ± {r2_std:.4f}"
    else:
        text_str = f"MSE  = {mse:.1E}\nR²   = {r2_value:.4f}"


    plt.text(
        0.02, 0.98,
        text_str,
        fontsize=14,
        color='black',
        ha='left',
        va='top',
        transform=plt.gca().transAxes,

    )
    plt.tight_layout(pad=0.5)
    plt.savefig(plot_name,dpi=200, facecolor='w',bbox_inches='tight', edgecolor='w')
    plt.close()

import torch
import torch.nn as nn
import csv
from torch_geometric.loader import DataLoader
from sklearn.model_selection import KFold
import os

os.makedirs("eachfold_plot", exist_ok=True)
os.makedirs("bestfold_plot", exist_ok=True)

def train(loader):
    model.train()
    total_loss = 0
    train_predictions = []
    train_targets = []

    for batch_idx, batch in enumerate(loader):
        batch.x.requires_grad = True

        optimizer.zero_grad()
        out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)

        loss = criterion(out, batch.y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
        optimizer.step()

        total_loss += loss.item()
        train_predictions.append(out)
        train_targets.append(batch.y)

    predictions = torch.cat(train_predictions, dim=0)
    targets = torch.cat(train_targets, dim=0)

    return total_loss / len(loader), predictions, targets


def test(loader):
    model.eval()
    total_loss = 0
    test_predictions = []
    test_targets = []

    with torch.no_grad():
        for batch_idx, batch in enumerate(loader):
            out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)

            loss = criterion(out, batch.y)
            total_loss += loss.item()
            test_predictions.append(out)
            test_targets.append(batch.y)

    predictions = torch.cat(test_predictions, dim=0)
    targets = torch.cat(test_targets, dim=0)

    return total_loss / len(loader), predictions, targets

all_preds_Cr, all_true_Cr = [], []
all_preds_Co, all_true_Co = [], []
all_preds_Temp, all_true_Temp = [], []

mse_Cr, mse_Co, mse_Temp = [], [], []
r2_Cr, r2_Co, r2_Temp = [], [], []

# Assuming you have `dataset` already (a list of Data objects)
kf = KFold(n_splits=5, shuffle=True, random_state=42)

num_epochs = 1000
fold_results = []

for fold, (train_idx, test_idx) in enumerate(kf.split(data_list)):
    print(f"\n==================== FOLD {fold+1} ====================")
    best_test_loss = float('inf')
    best_epoch = -1
    best_test_predictions = None
    best_test_targets = None
    # Split dataset
    train_dataset = [data_list[i] for i in train_idx]
    test_dataset = [data_list[i] for i in test_idx]

    train_data = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_data = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # Reinitialize model, optimizer, and loss function for each fold
    model = GraphSAGEModel(in_channels=1, out_channels=3)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    #criterion = nn.MSELoss()
    criterion = nn.L1Loss()


    # Logging and plotting data
    train_losses = []
    test_losses = []

# Training loop with testing

    for epoch in range(num_epochs):
        # Training
        train_loss, train_predictions, train_targets = train(train_data)
        train_losses.append(train_loss)

        print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}")

        # Testing
        test_loss, test_predictions, test_targets = test(test_data)
        test_losses.append(test_loss)

         # ===== Track best epoch =====
        if test_loss < best_test_loss:
            best_test_loss = test_loss
            best_epoch = epoch
            best_test_predictions = test_predictions.detach().clone()
            best_test_targets = test_targets.clone()

        pred_Cr_test, pred_Co_test, pred_temperature_test = test_predictions[:, 0], test_predictions[:, 1], test_predictions[:, 2]
        true_Cr_test, true_Co_test, true_temperature_test = test_targets[:, 0], test_targets[:, 1], test_targets[:, 2]

        # Normalized values (already between 0 and 1)
        true_temp_norm = true_temperature_test.numpy()
        pred_temp_norm = pred_temperature_test.detach().numpy()
        # Denormalized values
        true_temp_denorm = true_temp_norm * max_temperature
        pred_temp_denorm = pred_temp_norm * max_temperature
        # Calculate R²
        r2_temperature_value = r2_score(true_temp_norm, pred_temp_norm)
        # Define plot limits based on your data range
        lims_min = min(true_temp_denorm.min(), pred_temp_denorm.min())
        lims_max = max(true_temp_denorm.max(), pred_temp_denorm.max())
        # Call the function
        parity_plot(
            ground_truth=true_temp_denorm,
            prediction=pred_temp_denorm,
            lims_min=lims_min, lims_max=lims_max,
            plot_name=f"/bsuscratch/sulbhamalviya/New_Test_Folder/Test_1/eachfold_plot/parity_temp_fold{fold+1}.png",
            r2_value=r2_temperature_value,
            ground_truth_norm=true_temp_norm,
            prediction_norm=pred_temp_norm,
            label=f"Temperature - Fold {fold+1}"
        )
        pred_Cr = pred_Cr_test.detach().numpy()
        true_Cr = true_Cr_test.numpy()
        # Calculate R²
        r2_Cr_value = r2_score(true_Cr, pred_Cr)
        parity_plot(
            ground_truth=true_Cr,
            prediction=pred_Cr,
            lims_min=0, lims_max=1,
            plot_name=f"/bsuscratch/sulbhamalviya/New_Test_Folder/Test_1/eachfold_plot/parity_Cr_fold{fold+1}.png",
            r2_value=r2_Cr_value,label=f"Cr - Fold {fold+1}"

        )
        pred_Co = pred_Co_test.detach().numpy()
        true_Co = true_Co_test.numpy()
        # Calculate R²
        r2_Co_value = r2_score(true_Co, pred_Co)
        parity_plot(
            ground_truth=true_Co,
            prediction=pred_Co,
            lims_min=0, lims_max=1,
            plot_name=f"/bsuscratch/sulbhamalviya/New_Test_Folder/Test_1/eachfold_plot/parity_Co_fold{fold+1}.png",
            r2_value=r2_Co_value,
            label=f"Co - Fold {fold+1}"
        )
                    # Save CSV for this fold
        with open(f'losses_fold_{fold+1}.csv', mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(['Epoch', 'Train Loss', 'Test Loss'])
            num_epochs_to_write = min(len(train_losses), len(test_losses))
            for epoch in range(num_epochs_to_write):
                writer.writerow([epoch + 1, train_losses[epoch], test_losses[epoch]])

        # Split predictions for each target
    pred_Cr_best = best_test_predictions[:, 0].numpy()
    true_Cr_best = best_test_targets[:, 0].numpy()

    pred_Co_best = best_test_predictions[:, 1].numpy()
    true_Co_best = best_test_targets[:, 1].numpy()

    pred_Temp_best = best_test_predictions[:, 2].numpy()
    true_Temp_best = best_test_targets[:, 2].numpy()
    # Store for combined plots
    all_preds_Cr.append(pred_Cr_best)
    all_true_Cr.append(true_Cr_best)

    all_preds_Co.append(pred_Co_best)
    all_true_Co.append(true_Co_best)

    all_preds_Temp.append(pred_Temp_best)
    all_true_Temp.append(true_Temp_best)

    # Compute metrics for this fold
    mse_Cr.append(mean_squared_error(true_Cr_best, pred_Cr_best))
    r2_Cr.append(r2_score(true_Cr_best, pred_Cr_best))

    mse_Co.append(mean_squared_error(true_Co_best, pred_Co_best))
    r2_Co.append(r2_score(true_Co_best, pred_Co_best))

    mse_Temp.append(mean_squared_error(true_Temp_best, pred_Temp_best))
    r2_Temp.append(r2_score(true_Temp_best, pred_Temp_best))
    # Normalized values (already between 0 and 1)
    true_temp_norm = true_Temp_best
    pred_temp_norm = pred_Temp_best

    # Denormalized values
    true_temp_denorm = true_temp_norm * max_temperature
    pred_temp_denorm = pred_temp_norm * max_temperature

    # Calculate R²
    r2_temperature_best = r2_score(true_temp_norm, pred_temp_norm)

    # Define plot limits based on your data range
    lims_min = min(true_temp_denorm.min(), pred_temp_denorm.min())
    lims_max = max(true_temp_denorm.max(), pred_temp_denorm.max())

    # Call the function
    parity_plot(
    ground_truth=true_temp_denorm,
    prediction=pred_temp_denorm,
    lims_min=lims_min, lims_max=lims_max,
    plot_name=f"/bsuscratch/sulbhamalviya/New_Test_Folder/Test_1/bestfold_plot/parity_temp_fold{fold+1}.png",
    r2_value=r2_temperature_best,
    ground_truth_norm=true_temp_norm,
    prediction_norm=pred_temp_norm,
    label=f"Temperature-Fold{fold+1}Best Epoch"
    )
    pred_Cr = pred_Cr_best
    true_Cr = true_Cr_best

    # Calculate R²
    r2_Cr_best = r2_score(true_Cr, pred_Cr)
    parity_plot(
    ground_truth=true_Cr,
    prediction=pred_Cr,
    lims_min=0, lims_max=1,
    plot_name=f"/bsuscratch/sulbhamalviya/New_Test_Folder/Test_1/bestfold_plot/parity_Cr_fold{fold+1}.png",
    r2_value=r2_Cr_best,label=f"Cr-Fold{fold+1}Best Epoch"

    )
    pred_Co = pred_Co_best
    true_Co = true_Co_best

    # Calculate R²
    r2_Co_best = r2_score(true_Co, pred_Co)

    # Call the function
    # parity_plot(true_Co, pred_Co, 0, 1, 'parity_plot_Co.png',r2_value=r2_Co)
    parity_plot(
    ground_truth=true_Co,
    prediction=pred_Co,
    lims_min=0, lims_max=1,
    plot_name=f"/bsuscratch/sulbhamalviya/New_Test_Folder/Test_1/bestfold_plot/parity_Co_fold{fold+1}.png",
    r2_value=r2_Co_best,
    label=f"Co-Fold{fold+1}Best Epoch"
    )

import numpy as np

# Chromium
mse_Cr_mean, mse_Cr_std = np.mean(mse_Cr), np.std(mse_Cr)
r2_Cr_mean, r2_Cr_std = np.mean(r2_Cr), np.std(r2_Cr)

# Cobalt
mse_Co_mean, mse_Co_std = np.mean(mse_Co), np.std(mse_Co)
r2_Co_mean, r2_Co_std = np.mean(r2_Co), np.std(r2_Co)

# Temperature
mse_Temp_mean, mse_Temp_std = np.mean(mse_Temp), np.std(mse_Temp)
r2_Temp_mean, r2_Temp_std = np.mean(r2_Temp), np.std(r2_Temp)

print(f"Cr MSE: {mse_Cr_mean:.4f} ± {mse_Cr_std:.4f}, R²: {r2_Cr_mean:.4f} ± {r2_Cr_std:.4f}")
print(f"Co MSE: {mse_Co_mean:.4f} ± {mse_Co_std:.4f}, R²: {r2_Co_mean:.4f} ± {r2_Co_std:.4f}")
print(f"Temp MSE: {mse_Temp_mean:.4f} ± {mse_Temp_std:.4f}, R²: {r2_Temp_mean:.4f} ± {r2_Temp_std:.4f}")

import numpy as np
from sklearn.metrics import mean_squared_error, r2_score

# Combine all folds
all_preds_Temp_combined = np.concatenate(all_preds_Temp)
all_true_Temp_combined = np.concatenate(all_true_Temp)

# Compute mean ± std across folds
mse_Temp_mean, mse_Temp_std = np.mean(mse_Temp), np.std(mse_Temp)
r2_Temp_mean, r2_Temp_std = np.mean(r2_Temp), np.std(r2_Temp)

# Denormalize for plotting
all_true_Temp_denorm = all_true_Temp_combined * max_temperature
all_preds_Temp_denorm = all_preds_Temp_combined * max_temperature

lims_min = min(all_true_Temp_denorm.min(), all_preds_Temp_denorm.min())
lims_max = max(all_true_Temp_denorm.max(), all_preds_Temp_denorm.max())

parity_plot(
    ground_truth=all_true_Temp_denorm,
    prediction=all_preds_Temp_denorm,
    lims_min=lims_min, lims_max=lims_max,
    plot_name="combined_parity_temp.png",
    mse_mean=mse_Temp_mean, mse_std=mse_Temp_std,
    r2_mean=r2_Temp_mean, r2_std=r2_Temp_std,label='Temprature'
)

# ==================== COMBINED PARITY PLOT — Cr ====================
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score

# Combine all folds
all_preds_Cr_combined = np.concatenate(all_preds_Cr)
all_true_Cr_combined = np.concatenate(all_true_Cr)

# Compute mean ± std across folds
mse_Cr_mean, mse_Cr_std = np.mean(mse_Cr), np.std(mse_Cr)
r2_Cr_mean, r2_Cr_std = np.mean(r2_Cr), np.std(r2_Cr)


# Call parity plot function
parity_plot(
    ground_truth=all_true_Cr_combined,
    prediction=all_preds_Cr_combined,
    lims_min=0, lims_max=1,
    plot_name="combined_parity_Cr.png",
    mse_mean=mse_Cr_mean, mse_std=mse_Cr_std,
    r2_mean=r2_Cr_mean, r2_std=r2_Cr_std,label='Cr'
)

# ==================== COMBINED PARITY PLOT — Co ====================

all_preds_Co_combined = np.concatenate(all_preds_Co)
all_true_Co_combined = np.concatenate(all_true_Co)

mse_Co_mean, mse_Co_std = np.mean(mse_Co), np.std(mse_Co)
r2_Co_mean, r2_Co_std = np.mean(r2_Co), np.std(r2_Co)


parity_plot(
    ground_truth=all_true_Co_combined,
    prediction=all_preds_Co_combined,
    lims_min=0, lims_max=1,
    plot_name="combined_parity_Co.png",
    mse_mean=mse_Co_mean, mse_std=mse_Co_std,
    r2_mean=r2_Co_mean, r2_std=r2_Co_std,label='Co'
)

import pandas as pd
import matplotlib.pyplot as plt
import glob
import numpy as np
import os

os.makedirs("learning_curves", exist_ok=True)

# Load all folds’ CSVs
csv_files = sorted(glob.glob("losses_fold_*.csv"))
all_train, all_test = [], []

for file in csv_files:
    df = pd.read_csv(file)
    all_train.append(df["Train Loss"].values)
    all_test.append(df["Test Loss"].values)

# Convert to numpy arrays for averaging
all_train = np.array(all_train)
all_test = np.array(all_test)
epochs = np.arange(1, len(all_train[0]) + 1)

# Compute mean ± std
train_mean, train_std = all_train.mean(axis=0), all_train.std(axis=0)
test_mean, test_std = all_test.mean(axis=0), all_test.std(axis=0)

# Plot average curves
plt.figure(figsize=(5, 3), dpi=300)
plt.plot(epochs, train_mean, color='red', label='Train (mean)')
plt.fill_between(epochs, train_mean - train_std, train_mean + train_std, color='red', alpha=0.2)

plt.plot(epochs, test_mean, color='green', linestyle='--', label='Validation (mean)')
plt.fill_between(epochs, test_mean - test_std, test_mean + test_std, color='green', alpha=0.2)

plt.xlabel('Epoch', fontsize=14)
plt.ylabel('Loss', fontsize=14)
plt.legend(fontsize=12, loc='upper right', frameon=False)
plt.tight_layout()
plt.savefig('learning_curves/avg_learning_curve_pubready.png', dpi=300)
plt.close()

# Print first 10 values for true and predicted time and temperature
print("First 10 test true and predicted values for Time and Temperature:")
for i in range(10):
    #print(f"True Time: {true_time[i].item():.3f}, Pred Time: {pred_time[i].item():.3f}")
    print(f"True Temperature: {true_temperature_test[i].item():.3f}, Pred Temperature: {pred_temperature_test[i].item():.3f}")

print("First 10  test true and predicted values for Cr and Co:")
for i in range(10):
    print(f"True Cr: {true_Cr_test[i].item():.3f}, Pred Cr: {pred_Cr_test[i].item():.3f}")
    #print(f"True Co: {true_Co[i].item():.3f}, Pred Co: {pred_Co[i].item():.3f}")

print("First 10 test true and predicted values for Cr and Co:")
for i in range(10):
    #print(f"True Cr: {true_Cr[i].item():.3f}, Pred Cr: {pred_Cr[i].item():.3f}")
    print(f"True Co: {true_Co_test[i].item():.3f}, Pred Co: {pred_Co_test[i].item():.3f}")

# import matplotlib.pyplot as plt
# import pandas as pd

# # Load the data (example assumes you have it in a CSV file or as a DataFrame)
# # Replace 'file_path.csv' with your file path
# data = pd.read_csv('losses.csv')

# # Create output directory if it doesn't exist
# os.makedirs('logplot', exist_ok=True)

# # Extract values for plotting
# epochs = data['Epoch']
# train_loss = data['Train Loss']
# test_loss = data['Test Loss']

# # Create a log plot
# plt.figure(figsize=(10, 6))
# plt.semilogy(epochs, train_loss, label='Train Loss', color='blue')
# plt.semilogy(epochs, test_loss, label='Test Loss', color='red')

# # Add labels, title, and legend
# plt.xlabel('Epoch', fontsize=14)
# plt.ylabel('Loss (Log Scale)', fontsize=14)
# plt.title('Training and Testing Loss (Log Scale)', fontsize=16)
# plt.legend(fontsize=12)
# plt.grid(False, which="both", linestyle='--', linewidth=0.5)

# # Save the plot to a PDF file
# pdf_file_path = 'logplot/gnn_logplot.png'
# plt.savefig(pdf_file_path, format='png')  # Save as PDF

# # Show the plot
# #plt.show()
# plt.close()

