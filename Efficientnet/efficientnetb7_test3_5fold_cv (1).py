# -*- coding: utf-8 -*-
"""EfficientNetB7_test3_5fold_cv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GrvmhaWGfWW6FQgIwTXWjhfL08tAq3vI
"""

# # Required functions for loading and processing data set

# ## Loading libraries

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

import glob
#import cv2
import os, sys, pickle, gzip, time
import numpy as np
import pandas as pd

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

from tensorflow.keras.models import Sequential,Model
from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D,Activation,Dropout
from tensorflow.keras.layers import Dense,Flatten,Input,concatenate, GlobalAveragePooling2D
from tensorflow.keras.backend import sigmoid
from tensorflow.keras.utils import get_custom_objects
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint

from tensorflow.keras.applications import EfficientNetB7, VGG16, VGG19, imagenet_utils

from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.preprocessing.image import img_to_array

import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf

print("INFO............................")

print("python version:{}\n".format(sys.version))

print("TF version:{}\n".format(tf.__version__))

t1 = time.time()
# ## Loading images min-max

def load_image_minmax(inputPath):
    # initialize the list of column names in the CSV file and then
    # load it using Pandas
    cols = ["Cr","Co","temperature","min", "max", "image_name"]
    df = pd.read_excel(inputPath,header=None, names=cols)
    print(df["image_name"])
    # return the data frame
    return df


# ## Processing of image min_max

def process_image_minmax(df, train, test):
    # initialize the column names of the continuous data
    continuous = ["min", "max"]
    # performin min-max scaling each continuous feature column to
    # the range [0, 1]

#     cs = MinMaxScaler()
#     trainContinuous = cs.fit_transform(train[continuous])
#     testContinuous = cs.transform(test[continuous])
    trainContinuous = train[continuous]
    testContinuous = test[continuous]
    # construct our training and testing data points by concatenating
    # the categorical features with the continuous features
    # trainX = np.hstack([trainContinuous])
    # testX = np.hstack([testContinuous])
    # return the concatenated training and testing data
    trainX = np.array(trainContinuous)
    testX = np.array(testContinuous)
    return (trainX, testX)


# ## Extract image features by EfficientNetB4 convolutional layers

def create_features(df, inputPath, pre_model,size_img):

    images = []

    # loop over the images
    for i in df['image_name']:
        imagePath = os.path.join(inputPath,i)
    # load the input image and image is resized to 224x224 pixels
        image = load_img(imagePath, target_size=size_img)
        image = img_to_array(image)

        # preprocess the image by (1) expanding the dimensions and
        # (2) subtracting the mean RGB pixel intensity from the
        # ImageNet dataset
        image = np.expand_dims(image, axis=0)
        image = imagenet_utils.preprocess_input(image)

        # add the image to the batch
        images.append(image)

    x = np.vstack(images)
    features = pre_model.predict(x, batch_size=32)
    print(features.shape)
    features_flatten = features.reshape((features.shape[0], -1))

    return features_flatten
    #return features


# # Model development

# Swish defination
def swish(x, beta = 1):
    return (x * sigmoid(beta * x))

get_custom_objects().update({'swish': Activation(swish)})


# ## FC for extracted image features

def create_feature_model(dim, regress=False):
    model = Sequential()
    model.add(Dense(512, input_dim=dim))
    # model.add(BatchNormalization())
    # model.add(Dropout(0.7))
    # model.add(Dense(512))
    model.add(BatchNormalization())
    model.add(Activation(swish))
    model.add(Dropout(0.5))
    model.add(Dense(128))
    model.add(BatchNormalization())
    model.add(Activation(swish))

    # model.add(Dense(16, input_dim=dim, activation="relu"))
    # model.add(BatchNormalization(axis=-1))
    # model.add(Dropout(0.5))
    # model.add(Dense(4, activation="relu"))
    # check to see if the regression node should be added
    if regress:
        model.add(Dense(1, activation="linear"))
    # return our model
    return model


# # building 2 fully connected layer
# x = model.output

# x = BatchNormalization()(x)
# x = Dropout(0.7)(x)

# x = Dense(512)(x)
# x = BatchNormalization()(x)
# x = Activation(swish_act)(x)
# x = Dropout(0.5)(x)

# x = Dense(128)(x)
# x = BatchNormalization()(x)
# x = Activation(swish_act)(x)


# ## FC for combining output of CNN with numeric data (image min_max)

def create_mlp(dim, regress=False):
    # define our MLP network
    model = Sequential()
    model.add(Dense(8, input_dim=dim, activation="relu"))
    model.add(Dense(4, activation="relu"))
    # check to see if the regression node should be added
    if regress:
        model.add(Dense(1, activation="linear"))
    # return our model
    return model
def plot_loss(history, plot_name):
    plt.figure()

    # If input is a History object, get .history dict
    if hasattr(history, 'history'):
        h = history.history
    else:  # assume it's already a dict
        h = history

    plt.plot(h['loss'], label='Train loss', color='red')
    plt.plot(h['val_loss'], label='Validation loss', color='green', linestyle='--')
    plt.ylim([0, 100])
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(loc='upper right', fontsize=10)
    plt.grid(False)
    plt.title(plot_name)
    # ✅ Force x-axis ticks at every 100 epochs
    num_epochs = len(h['loss'])
    tick_positions = np.arange(0, num_epochs + 1, 100)
    ax = plt.gca()
    ax.set_xticks(tick_positions)
    ax.set_xticklabels([str(int(x)) for x in tick_positions], fontsize=10)

    # ✅ Expand margins slightly so labels fit
    plt.margins(x=0.01)
    plt.tight_layout()
    plt.savefig(plot_name, dpi=120, facecolor='w', edgecolor='w')
    plt.close()

path="/bsuscratch/sulbhamalviya/Amirs_code_testing/Efficientnet/clean_data/"
# pretrained_model = EfficientNetB7(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
# size_img = (224, 224)
# num_used_layers = [25, 108, 212, 286, 346, 406, 464, 509, 613, 673, 806, 810]
#num_used_layers = [4, 25, 108, 286, 509,810]
from tensorflow.keras.applications import EfficientNetB6

# Load pretrained EfficientNet-B6
pretrained_model = EfficientNetB6(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Image size
size_img = (224, 224)

# Selected layers from EfficientNet-B6
num_used_layers = [96, 111, 142, 231, 304, 319, 362, 392, 496, 556, 631, 659, 663]

#epch = 1

print("[INFO] loading images min-max...")
inputPath = os.path.sep.join([path, "clean_data_no_time.xlsx"])
df = load_image_minmax(inputPath)
image_names = set(df["image_name"].values)


# List all files in the folder and filter only those that match CSV
all_image_files = os.listdir(path)
filtered_images = [img for img in all_image_files if img in image_names]

print("Data shape")
print(df.shape)

for i in num_used_layers:
    print('--------------------------------------------------------------------------------------------------------------')
    print('--------------------------------------------------------------------------------------------------------------')
    print('[INFO] AFTER LAYER '+str(i))
    print('--------------------------------------------------------------------------------------------------------------')
    print('--------------------------------------------------------------------------------------------------------------')
    print("[INFO] loading spinodal images...")
    pre_model = Model(inputs=pretrained_model.inputs, outputs=pretrained_model.layers[i].output)
    images = create_features(df, path, pre_model, size_img)

    print('Image shape layer' +str(i))
    print(images.shape)

print(f"Images listed in Excel: {len(image_names)}")
print(f"Images matched in folder: {len(filtered_images)}")

print("[INFO] processing data...")

colY = ["temperature", "Cr", "Co"]

# Keep original temperature for reference
df["temperature_orig"] = df["temperature"]

# Scale temperature for model input
maxTemp = df["temperature_orig"].max()
df["temperature_scaled"] = df["temperature_orig"] / maxTemp

print("Original max:", df["temperature_orig"].max())  # 970
print("Scaled max:", df["temperature_scaled"].max())   # 1.0

# Prepare labels array (using scaled temperature)
Y = np.array(df[["temperature_scaled", "Cr", "Co"]])

def parity_plot(
    ground_truth_norm, prediction_norm,
    max_value=None, lims_min=None, lims_max=None,
    plot_name="parity_plot.png", label="Target",
    custom_ticks=False, tick_interval=20,
    mse_mean=None, mse_std=None, r2_mean=None, r2_std=None
):
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.metrics import r2_score, mean_squared_error

    # Convert to numpy
    ground_truth_norm = np.array(ground_truth_norm)
    prediction_norm = np.array(prediction_norm)

    # Denormalize if needed
    if max_value is not None:
        ground_truth_plot = ground_truth_norm * max_value
        prediction_plot = prediction_norm * max_value
    else:
        ground_truth_plot = ground_truth_norm
        prediction_plot = prediction_norm

    # Determine limits
    if lims_min is None:
        lims_min = ground_truth_plot.min() - 10
    if lims_max is None:
        lims_max = ground_truth_plot.max() + 10
    lims = [lims_min, lims_max]

    plt.figure(figsize=(3.5,3.5), dpi=200)
    plt.scatter(ground_truth_plot, prediction_plot, color='black', s=5)
    plt.plot(lims, lims, 'k--', linewidth=1)
    plt.xlim(lims)
    plt.ylim(lims)

    if custom_ticks:
        ticks = np.arange(lims_min, lims_max+1, tick_interval)
        plt.xticks(ticks)
        plt.yticks(ticks)

    plt.xlabel("Ground Truth", fontsize=25, labelpad=4)
    plt.ylabel("Prediction", fontsize=25, labelpad=4)
    plt.title(label, fontsize=20, loc="center")
    plt.tick_params(axis='both', which='both', length=0)

    # Display metrics
    if mse_mean is not None and r2_mean is not None:
        # Combined plot: show mean ± std
        plt.text(
            0.02, 0.98,
            f"R²= {r2_mean:.4f} ± {r2_std:.4f}\nMSE= {mse_mean:.1E} ± {mse_std:.1E}",
            fontsize=12, color='black', ha='left', va='top', transform=plt.gca().transAxes
        )
    else:
        # Single-fold plot: show MSE & R²
        r2 = r2_score(ground_truth_norm, prediction_norm)
        mse = mean_squared_error(ground_truth_norm, prediction_norm)
        plt.text(
            0.02, 0.98,
            f"R² = {r2:.4f}\nMSE = {mse:.1E}",
            fontsize=12, color='black', ha='left', va='top', transform=plt.gca().transAxes
        )
    plt.tight_layout(pad=0.5)
    plt.savefig(plot_name, dpi=300)
    plt.close()

from sklearn.model_selection import KFold
from sklearn.metrics import r2_score, mean_squared_error
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
import pickle, time
import os

#os.makedirs("combined_outputs", exist_ok=True)
os.makedirs("eachfold_plot", exist_ok=True)

# ========== FUNCTION TO BUILD MODEL ==========
def build_model():
    # ---------------- YOUR ORIGINAL CODE (unchanged) ----------------
    mlp = create_mlp(x_attr_train.shape[1], regress=False)
    cnn = create_feature_model(x_img_train.shape[1], regress=False)

    mlp_input = Input(shape=(x_attr_train.shape[1],))
    cnn_input = Input(shape=(x_img_train.shape[1],))

    mlp_output = mlp(mlp_input)
    cnn_output = cnn(cnn_input)
    combinedInput = concatenate([mlp.output, cnn.output])

    x = Dense(32, activation="relu")(combinedInput)
    x = Dense(16, activation="relu")(x)
    x = Dense(3, activation="linear")(x)
    model = Model(inputs=[mlp.input, cnn.input], outputs=x)

    opt = Adam(learning_rate=1e-3)
    model.compile(loss="mean_absolute_percentage_error", optimizer=opt)
    return model

# ========== SETUP ==========
kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold_no = 1
val_scores = []
t1 = time.time()
all_histories = []  # <-- to store history for averaging

# To store predictions and targets across folds
all_preds_temp, all_targets_temp = [], []
all_preds_cr, all_targets_cr = [], []
all_preds_co, all_targets_co = [], []

# To store per-fold MSE and R²
mse_temp_scores, r2_temp_scores = [], []
mse_cr_scores, r2_cr_scores       = [], []
mse_co_scores, r2_co_scores       = [], []


# ========== 5-FOLD TRAINING LOOP ==========
for train_index, val_index in kf.split(df):
    print(f"\n[INFO] Starting Fold {fold_no}...")

    # Split data for this fold
    x_train_df, x_val_df = df.iloc[train_index], df.iloc[val_index]
    y_train, y_val = Y[train_index], Y[val_index]
    x_img_train, x_img_val = images[train_index], images[val_index]

    # Scale continuous columns (e.g., 'min' and 'max') inside the fold
    continuous = ["min", "max"]
    cs = MinMaxScaler()
    x_train_scaled = cs.fit_transform(x_train_df[continuous])
    x_val_scaled = cs.transform(x_val_df[continuous])

    # Convert to numpy arrays for model input
    x_attr_train = np.array(x_train_scaled)
    x_attr_val = np.array(x_val_scaled)

    # Build and compile fresh model
    model = build_model()

    # Checkpoint for best validation loss
    best_weights = f'outputs/weights_best_fold{fold_no}.keras'
    checkpoint = ModelCheckpoint(best_weights, monitor='val_loss', verbose=1, save_best_only=True, mode='min')


    print(f"[INFO] Training model for Fold {fold_no} ...")
    history = model.fit(
        x=[x_attr_train, x_img_train], y=y_train,
        validation_data=([x_attr_val, x_img_val], y_val),
        epochs=1000, batch_size=32,
        callbacks=[checkpoint], verbose=0
    )

    # Save fold history for averaging later
    all_histories.append(history.history)

    # Load best weights before validation evaluation
    model.load_weights(best_weights)

    # Predict on validation data
    preds_val = model.predict([x_attr_val, x_img_val])
    # ===== STEP 2 FOR ALL 3 TARGETS =====
# Temperature
# Inside the fold loop, for Temperature
    mse_temp = mean_squared_error(y_val[:,0], preds_val[:,0])
    r2_temp  = r2_score(y_val[:,0], preds_val[:,0])

    mse_temp_scores.append(mse_temp)   # store per fold
    r2_temp_scores.append(r2_temp)

    all_preds_temp.extend(preds_val[:,0])
    all_targets_temp.extend(y_val[:,0])

# Single-fold parity plot for temperature
    parity_plot(
        ground_truth_norm=y_val[:,0],
        prediction_norm=preds_val[:,0],
        max_value=maxTemp,  # denormalize if needed
        plot_name=f"eachfold_plot/parity_temp_fold{fold_no}.png",
        label=f"Temperature - Fold {fold_no}",
        custom_ticks=True, tick_interval=20
    )

 # ========== Cr ==========
    mse_cr = mean_squared_error(y_val[:,1], preds_val[:,1])
    r2_cr  = r2_score(y_val[:,1], preds_val[:,1])

    mse_cr_scores.append(mse_cr)       # store MSE per fold
    r2_cr_scores.append(r2_cr)

    all_preds_cr.extend(preds_val[:,1])
    all_targets_cr.extend(y_val[:,1])
# Single-fold parity plot for Cr
    parity_plot(
        ground_truth_norm=y_val[:,1],
        prediction_norm=preds_val[:,1],
        lims_min=0, lims_max=1,
        plot_name=f"eachfold_plot/parity_cr_fold{fold_no}.png",
        label=f"Cr - Fold {fold_no}"
    )
# ========== C0 ==========
    mse_co = mean_squared_error(y_val[:,2], preds_val[:,2])
    r2_co  = r2_score(y_val[:,2], preds_val[:,2])

    mse_co_scores.append(mse_co)       # store MSE per fold
    r2_co_scores.append(r2_co)

    all_preds_co.extend(preds_val[:,2])
    all_targets_co.extend(y_val[:,2])

# Single-fold parity plot for Cr
    parity_plot(
        ground_truth_norm=y_val[:,2],
        prediction_norm=preds_val[:,2],
        lims_min=0, lims_max=1,
        plot_name=f"eachfold_plot/parity_co_fold{fold_no}.png",
        label=f"Co - Fold {fold_no}"
    )

    # ========== SAVE HISTORY ==========
    picklefile = f'outputs/model_history_fold{fold_no}.pkl'
    with open(picklefile, 'wb') as f:
        pickle.dump(history.history, f)
    print(f'History saved to: {picklefile}')

    fold_no += 1

# ========== AFTER ALL FOLDS ==========
print("\n[INFO] Cross-validation complete.")

# ========== AVERAGE LEARNING CURVE ==========
num_epochs = len(all_histories[0]['loss'])
avg_history = {'loss': [], 'val_loss': []}
for epoch in range(num_epochs):
    avg_loss = np.mean([h['loss'][epoch] for h in all_histories])
    avg_val_loss = np.mean([h['val_loss'][epoch] for h in all_histories])
    avg_history['loss'].append(avg_loss)
    avg_history['val_loss'].append(avg_val_loss)

plot_loss(avg_history, 'devCNN_avgFold')  # <-- averaged learning curve

print(f'Total time taken: {time.time()-t1:.2f} seconds')

# Temperature
mse_temp_mean, mse_temp_std = np.mean(mse_temp_scores), np.std(mse_temp_scores)
r2_temp_mean, r2_temp_std   = np.mean(r2_temp_scores), np.std(r2_temp_scores)

# Cr
mse_cr_mean, mse_cr_std = np.mean(mse_cr_scores), np.std(mse_cr_scores)
r2_cr_mean, r2_cr_std   = np.mean(r2_cr_scores), np.std(r2_cr_scores)

# Co
mse_co_mean, mse_co_std = np.mean(mse_co_scores), np.std(mse_co_scores)
r2_co_mean, r2_co_std   = np.mean(r2_co_scores), np.std(r2_co_scores)

# Print results
print(f"Temperature: MSE = {mse_temp_mean:.4f} ± {mse_temp_std:.4f}, R² = {r2_temp_mean:.4f} ± {r2_temp_std:.4f}")
print(f"Cr: MSE = {mse_cr_mean:.4f} ± {mse_cr_std:.4f}, R² = {r2_cr_mean:.4f} ± {r2_cr_std:.4f}")
print(f"Co: MSE = {mse_co_mean:.4f} ± {mse_co_std:.4f}, R² = {r2_co_mean:.4f} ± {r2_co_std:.4f}")

# Temperature
parity_plot(
    all_targets_temp, all_preds_temp,
    max_value=maxTemp,
    plot_name="combined_parity_temp.png",
    label="Temperature ",
    custom_ticks=True, tick_interval=20,
    mse_mean=mse_temp_mean, mse_std=mse_temp_std,
    r2_mean=r2_temp_mean, r2_std=r2_temp_std
)

# Cr
parity_plot(
    all_targets_cr, all_preds_cr,
    lims_min=0, lims_max=1,
    plot_name="combined_parity_cr.png",
    label="Cr ",
    mse_mean=mse_cr_mean, mse_std=mse_cr_std,
    r2_mean=r2_cr_mean, r2_std=r2_cr_std
)

# Co
parity_plot(
    all_targets_co, all_preds_co,
    lims_min=0, lims_max=1,
    plot_name="combined_parity_co.png",
    label="Co",
    mse_mean=mse_co_mean, mse_std=mse_co_std,
    r2_mean=r2_co_mean, r2_std=r2_co_std
)

